{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "code_folding": [
     1
    ],
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lecture 3: Matrix norms, scalar products, unitary matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recap of the previous lecture\n",
    "\n",
    "- Matrix multiplication is the core of NLA\n",
    "- This is all about computer memory hierarchy\n",
    "- Concept of block algorithms\n",
    "- (Advanced topic) Strassen and trilinear\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Matrices and norms\n",
    "\n",
    "Recall vector norms that allow to evaluate distance between two vectors or how large are elements of a vector.\n",
    "\n",
    "How to generalize this concept to matrices?\n",
    "\n",
    "A trivial answer is that there is no big difference between matrices and vectors, and here comes the simplest matrix norm - **Frobenius** norm:\n",
    "$$\n",
    "  \\Vert A \\Vert_F = \\Big(\\sum_{i=1}^n \\sum_{j=1}^m |a_{ij}|^2\\Big)^{1/2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Matrix norms\n",
    "$\\Vert \\cdot \\Vert$ is called a **matrix norm** if it is a vector norm on the linear space of $n \\times m$ matrices:\n",
    "1. $\\|A\\| \\geq 0$\n",
    "2. $\\|A\\| = 0$ iff $A = O$\n",
    "3. $\\|\\alpha A\\| = |\\alpha| \\|A\\|$\n",
    "4. $\\|A+B\\| \\leq \\|A\\| + \\|B\\|$\n",
    "\n",
    "Additionally some matrix norms can satisfy the *submultiplicative property*\n",
    "\n",
    "* $\\Vert A B \\Vert \\leq \\Vert A \\Vert \\Vert B \\Vert$\n",
    "\n",
    "The submultiplicative property is needed in many places, for example in the estimates for the error of solution of linear systems (we will cover this subject later). \n",
    "\n",
    "Example of a non-submultiplicative norm: Chebyshev norm $\\|A\\|_C = \\displaystyle{\\max_{i,j}}\\, |a_{ij}|$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Operator norms\n",
    "The most important class of the norms is the class of **operator norms**. Mathematically, they are defined as\n",
    "\n",
    "$$\n",
    "    \\Vert A \\Vert_{*,**} = \\sup_{x \\ne 0} \\frac{\\Vert A x \\Vert_*}{\\Vert x \\Vert_{**}},\n",
    "$$\n",
    "\n",
    "where $\\Vert \\cdot \\Vert_*$ and $\\| \\cdot \\|_{**}$ are **vector norms**.\n",
    "\n",
    "It is easy to check that operator norms are submultiplicative.\n",
    "\n",
    "Frobenius norm is not an operator norm, i.e. you can not find $\\Vert \\cdot \\Vert_*$ and $\\| \\cdot \\|_{**}$ that induce it. A nontrivial fact."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Matrix $p$-norms\n",
    "\n",
    "Important case of operator norms are matrix $p$-norms, which are defined for $\\|\\cdot\\|_* = \\|\\cdot\\|_{**} = \\|\\cdot\\|_p$. <br>\n",
    " Among all $p$-norms three norms are the most common ones:  \n",
    "\n",
    "- $p = 2, \\quad$ spectral norm, denoted by $\\Vert A \\Vert_2$.\n",
    "- $p = \\infty, \\quad \\Vert A \\Vert_{\\infty} = \\max_i \\sum_j |a_{ij}|$.\n",
    "- $p = 1, \\quad \\Vert A \\Vert_{1} = \\max_j \\sum_i |a_{ij}|$.\n",
    "\n",
    "Let us check it for $p=\\infty$ on a blackboard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Spectral norm\n",
    "Spectral norm, $\\Vert A \\Vert_2$ is one of the most used matrix norms (along with the Frobenius norm). It can not be computed directly from the entries using a simple formula, like the Euclidean norm, however, there are efficient algorithms to compute it.  It is directly related to the **singular value decomposition** (SVD) of the matrix. It holds\n",
    "\n",
    "$$\n",
    "   \\Vert A \\Vert_2 = \\sigma_1(A) = \\sqrt{\\lambda_\\max(A^*A)}\n",
    "$$\n",
    "\n",
    "where $\\sigma_1(A)$ is the largest singular value of the matrix $A$ and $^*$ is a *conjugate transpose* of the matrix. We will soon learn all about the SVD. Meanwhile, we can already compute the norm in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spectral: 19.4348433329 \n",
      "Frobenius: 99.4981700135 \n",
      "1-norm 94.6994776999 \n",
      "infinity 92.6149263332\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "n = 100\n",
    "a = np.random.randn(n, n) #Random n x n matrix\n",
    "s1 = np.linalg.norm(a, 2) #Spectral\n",
    "s2 = np.linalg.norm(a, 'fro') #Frobenius\n",
    "s3 = np.linalg.norm(a, 1) #1-norm\n",
    "s4 = np.linalg.norm(a, np.inf) #It was trick to find the infinity\n",
    "print 'Spectral:', s1, '\\nFrobenius:', s2, '\\n1-norm', s3, '\\ninfinity', s4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Examples\n",
    "\n",
    "Several examples where matrix norms arise:\n",
    "* $\\displaystyle{\\min_{\\mathrm{rank}(A_r)=r}}\\| A - A_r\\|$ - finding best rank-r approximation. SVD helps to solve this problem for $\\|\\cdot\\|_2$ and $\\|\\cdot\\|_F$.\n",
    "\n",
    "\n",
    "* $\\displaystyle{\\min_B}\\| P_\\Omega \\odot(A - B)\\| + \\mathrm{rank}(B)$ - matrix completion. $(P_\\Omega)_{ij} = 1$ if $i,j\\in\\Omega$ and $0$ otherwise. $\\odot$ denotes Hadamard product (elementwise)\n",
    "\n",
    "\n",
    "* $\\displaystyle{\\min_{B,C\\geq 0}} \\|A - BC\\|$, nonnegative matrix factorization. Symbol $B\\geq0$ here means that all elements of $B$ are nonnegative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Scalar product\n",
    "While norm is a measure of distance, the scalar product takes angle into account.  \n",
    "\n",
    "The scalar product is defined as\n",
    "$$\n",
    "   (x, y) =  x^* y = \\sum_{i=1}^n \\overline{x}_i y_i ,\n",
    "$$\n",
    "where $\\overline{x}$ denotes the *complex conjugate* of $x$. The Euclidean norm is then\n",
    "\n",
    "$$\n",
    "   \\Vert x \\Vert_2 = (x, x)^{1/2},\n",
    "$$\n",
    "\n",
    "or it is said the the norm is **induced** by scalar product.  \n",
    "\n",
    "**Remark**. The angle between two vectors is defined as\n",
    "$$\n",
    "   \\cos \\phi = \\frac{(x, y)}{\\Vert x \\Vert_2 \\Vert y \\Vert_2} \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "An important property of the scalar product is the **Cauchy-Schwarz-Bunyakovski inequality**:\n",
    "$$\n",
    "    |(x, y)| \\leq \\Vert x \\Vert_2 \\Vert y \\Vert_2,\n",
    "$$\n",
    "and thus the angle between two vectors is defined properly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Matrices preserving the norm\n",
    "\n",
    "For stability it is really important that the error does not grow after we apply some transformations. Suppose that you approximately get a vector,  \n",
    "\n",
    "$$\n",
    "  \\Vert x - \\widehat{x} \\Vert \\leq \\varepsilon.\n",
    "$$\n",
    "Let  final result be (some) linear transformation of $x$:  \n",
    "$$\n",
    "   y = U x, \\quad \\widehat{y} = U \\widehat{x}.\n",
    "$$\n",
    "If we want to estimate a difference between $\\widehat{y}$ and $y$:  \n",
    "\n",
    "$$\n",
    "   \\Vert y - \\widehat{y} \\Vert = \\Vert U ( x - \\widehat{x}) \\Vert  \\stackrel{\\text{?}}{\\leq} \\varepsilon\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The question is for which kind of matrices the norm of the vector **will not change**.  \n",
    "\n",
    "For the euclidean norm $\\|\\cdot\\|_2$ the answer is **unitary** (or orthogonal in the real case) matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Unitary (orthogonal) matrices\n",
    "Let $U$ be complex $n \\times r$ matrix, and $\\Vert U z \\Vert_2 = \\Vert z \\Vert_2$ for all $z$. This can happen if and only if  \n",
    "\n",
    "$$\n",
    "   U^* U = I_r,\n",
    "$$\n",
    "\n",
    "where $I_r$ is an **identity matrix** $r\\times r$.\n",
    "\n",
    "Indeed, $$\\Vert Uz \\Vert_2^2 = (Uz, Uz) = (Uz)^* Uz = z^* (U^* U) z = z^* z = \\|z\\|_2^2,$$ \n",
    "\n",
    "which can only hold if $U^* U = I_r$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Complex $n\\times n$ square matrix is called **unitary** if\n",
    "$$\n",
    "    U^*U = UU^* = I_n,\n",
    "$$\n",
    "which means that columns and rows of unitary matrices both form orthonormal basis in $\\mathbb{C}^{n}$.\n",
    "\n",
    "For rectangular matrices of size $m\\times n$ ($n\\not= m$) only one of the equalities can hold\n",
    "\n",
    "$$\n",
    "   U^*U = I_n \\text{ - left unitary for $m>n$} \\quad \\text{or} \\quad UU^* = I_m \\text{ - right unitary for $m<n$}.\n",
    "$$\n",
    "\n",
    "In case of real matrices $U^* = U^T$ and matrices\n",
    "$$\n",
    "    U^TU = UU^T = I\n",
    "$$\n",
    "are called **orthogonal**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Unitary matrices\n",
    "\n",
    "Important property: **a product of two unitary matrices is a unitary matrix:**  \n",
    "\n",
    "$$(UV)^* UV = V^* (U^* U) V = V^* V = I,$$\n",
    "\n",
    "Later we will show that there are types of matrices (Householder reflections and Givens rotations) composition of which is able to produce arbitrary unitary matrix. <br>\n",
    "This idea is a core of some algorithms (e.g. QR decomposition)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Examples of unitary matrices\n",
    "There are two important classes of unitary matrices, using composition of which we can make any unitary matrix:\n",
    "1. Householder matrices\n",
    "2. Givens (Jacobi) matrices\n",
    "\n",
    "Other important examples are\n",
    "* Permutation matrix $P$ whose rows (columns) are permutation of rows (columns) of the identity matrix.\n",
    "* Fourier matrix $F_n = \\frac{1}{\\sqrt{n}} \\{ e^{-i\\frac{2\\pi kl}{n}}\\}_{k,l=0}^{n-1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Householder matrices\n",
    "Householder matrix is the matrix of the form \n",
    "\n",
    "$$H \\equiv H(v) = I - 2 vv^*,$$\n",
    "\n",
    "where $v$ is an $n \\times 1$ column and $v^* v = 1$. Can you show that $H$ is unitary and Hermitian ($H^* = H$)?  It is also a reflection:\n",
    "\n",
    "$$ Hx = x - 2(v^* x) v$$\n",
    "\n",
    "<img src=\"householder.jpeg\">  \n",
    "A simple proof: $H^* H = (I - 2 vv^*)(I - 2 v v^*) = I$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Important property of Householder reflection\n",
    "\n",
    "A nice property of Housholder transformation is that it can zero all elements of a vector except for the first one:\n",
    "$$\n",
    "    H \\begin{bmatrix} \\times \\\\ \\times \\\\ \\times \\\\ \\times  \\end{bmatrix} = \n",
    "      \\begin{bmatrix} \\times \\\\ 0 \\\\ 0 \\\\ 0  \\end{bmatrix}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "*Proof.* Let $e_1 = (1,0,\\dots, 0)^T$, then we want to find $v$ such that\n",
    "\n",
    "$$\n",
    "   H x = x - 2(v^* x) v = \\alpha e_1,\n",
    "$$\n",
    "\n",
    "where $\\alpha$ is unknown constant. \n",
    "Multiplying by $x^*$ we get\n",
    "\n",
    "$$\n",
    "    x^* x - 2 (v^* x) x^* v = \\alpha x_1;\n",
    "$$\n",
    "$$\n",
    "    \\|x\\|_2^2 - 2 (v^* x)^2 = \\alpha x_1\n",
    "$$\n",
    "$$\n",
    "    (v^* x)^2 = \\frac{\\|x\\|_2^2 - \\alpha x_1}{2}\n",
    "$$\n",
    "\n",
    "Since $v^*v =1$, $v = \\dfrac{x-\\alpha e_1}{2 v^* x}$ and $(v^* x)^2 = \\dfrac{\\|x\\|_2^2 - \\alpha x_1}{2}$ we obtain\n",
    "\n",
    "$$\n",
    "    1 = v^* v = \\dfrac{x^*x  - 2\\alpha x_1 + \\alpha^2}{4 (v^* x)^2} = \\dfrac{\\|x\\|_2^2  - 2\\alpha x_1 + \\alpha^2}{2(\\|x\\|_2^2 - \\alpha x_1)};\n",
    "$$\n",
    "$$\n",
    "     \\|x\\|_2^2  - 2\\alpha x_1 + \\alpha^2 = 2\\|x\\|_2^2 - 2\\alpha x_1;\n",
    "$$\n",
    "$$\n",
    "    \\alpha^2 = \\|x\\|_2^2;\n",
    "$$\n",
    "$$\n",
    "    \\alpha = \\pm \\|x\\|_2.\n",
    "$$\n",
    "\n",
    "So, $v$ exists and equals\n",
    "$$\n",
    "    v = \\dfrac{x \\pm \\|x\\|_2 e_1}{2v^* x} = \\dfrac{x \\pm \\|x\\|_2 e_1}{\\pm\\sqrt{2(\\|x\\|_2^2 \\mp \\|x\\|_2 x_1)}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Housholder algorithm for QR decomposition\n",
    "\n",
    "Using the obtained property we can make arbitrary matrix $A$ lower triangular:\n",
    "$$\n",
    "    H_2 H_1 A = \n",
    "    \\begin{bmatrix}\n",
    "        \\times & \\times & \\times & \\times \\\\ \n",
    "        0 & \\times & \\times & \\times  \\\\ \n",
    "        0 & 0 & \\boldsymbol{\\times} & \\times\\\\ \n",
    "        0 &0 & \\boldsymbol{\\times} & \\times  \\\\ \n",
    "        0 &0 & \\boldsymbol{\\times} & \\times \n",
    "    \\end{bmatrix}\n",
    "$$\n",
    "then finding $H_3=\\text{diag}(I_2, \\tilde H_3)$ such that\n",
    "$$\n",
    "    \\tilde H_3 \\begin{bmatrix} \\boldsymbol{\\times}\\\\ \\boldsymbol{\\times} \\\\ \\boldsymbol{\\times}  \\end{bmatrix} = \n",
    "      \\begin{bmatrix} \\times \\\\ 0 \\\\ 0  \\end{bmatrix}.\n",
    "$$\n",
    "we get\n",
    "$$\n",
    "    H_3 H_2 H_1 A = \n",
    "    \\begin{bmatrix}\n",
    "        \\times & \\times & \\times & \\times \\\\ \n",
    "        0 & \\times & \\times & \\times  \\\\ \n",
    "        0 & 0 & {\\times} & \\times\\\\ \n",
    "        0 &0 & 0 & \\times  \\\\ \n",
    "        0 &0 & 0 & \\times \n",
    "    \\end{bmatrix}\n",
    "$$\n",
    "Finding $H_4$ by analogy we arrive at upper-triangular matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "** Corollary:** (QR decomposition) Every $A\\in \\mathbb{C}^{n\\times m}$, can be represented as\n",
    "$$\n",
    "    A = QR,\n",
    "$$\n",
    "where $Q$ is unitary and $R$ is upper triangular."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Givens (Jacobi) matrix\n",
    "A Givens matrix is a matrix  \n",
    "\n",
    "$$\n",
    "    G = \\begin{bmatrix}\n",
    "          \\cos \\alpha & -\\sin \\alpha \\\\\n",
    "          \\sin \\alpha & \\cos \\alpha\n",
    "        \\end{bmatrix},\n",
    "$$\n",
    "\n",
    "which is a rotation. For a general case, we select two $(i, j)$ planes and rotate vector $x$  \n",
    "$$\n",
    "x' = G x,\n",
    "$$\n",
    "\n",
    "only in $i$-th and $j$-th positions:\n",
    "\n",
    "$$\n",
    "    x'_i =  x_i\\cos \\alpha - x_j\\sin \\alpha , \\quad x'_j = x_i \\sin \\alpha  +  x_j\\cos\\alpha,\n",
    "$$\n",
    "\n",
    "with all other $x_i$ remain unchanged.\n",
    "Therefore, we can make elements in the $j$-th  position zero by choosing $\\alpha$ such that\n",
    "$$\n",
    "     \\cos \\alpha = \\frac{x_i}{\\sqrt{x_i^2 + x_j^2}}, \\quad \\sin \\alpha = -\\frac{x_j}{\\sqrt{x_i^2 + x_j^2}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## QR via Givens rotations\n",
    "\n",
    "Similarly we can make matrix upper-triagular using Givens rotations:\n",
    "\n",
    "$$\n",
    "    \\begin{bmatrix} \n",
    "        \\times & \\times & \\times \\\\\n",
    "        \\bf{*} & \\times & \\times \\\\\n",
    "        \\bf{*} & \\times & \\times \n",
    "    \\end{bmatrix}\n",
    "    \\to \n",
    "    \\begin{bmatrix} \n",
    "        * & \\times & \\times \\\\\n",
    "        * & \\times & \\times \\\\\n",
    "        0 & \\times & \\times \n",
    "    \\end{bmatrix}\n",
    "    \\to\n",
    "    \\begin{bmatrix} \n",
    "        \\times & \\times & \\times \\\\\n",
    "        0 & * & \\times \\\\\n",
    "        0 & * & \\times \n",
    "    \\end{bmatrix}\n",
    "    \\to\n",
    "    \\begin{bmatrix} \n",
    "        \\times & \\times & \\times \\\\\n",
    "        0 & \\times & \\times \\\\\n",
    "        0 & 0 & \\times \n",
    "    \\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Givens vs. Housholder\n",
    "\n",
    "- Housholder is useful for dense matrices (complexity is $\\approx$ twice smaller than for Jacobi) and we need to zero large number of elements.\n",
    "- Givens rotations are more suitable for sparse matrice or parallel machines as it acts locally on elements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary \n",
    "- Unitary matrices preserve the norm\n",
    "- There are two \"basic\" classes of unitary matrices, Householder and Givens.\n",
    "- Every unitary matrix can be represented as a product of those."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Next week\n",
    "- SVD\n",
    "- You got PSet 1\n",
    "- Think of course projects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### Questions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<link href='http://fonts.googleapis.com/css?family=Fenix' rel='stylesheet' type='text/css'>\n",
       "<link href='http://fonts.googleapis.com/css?family=Alegreya+Sans:100,300,400,500,700,800,900,100italic,300italic,400italic,500italic,700italic,800italic,900italic' rel='stylesheet' type='text/css'>\n",
       "<link href='http://fonts.googleapis.com/css?family=Source+Code+Pro:300,400' rel='stylesheet' type='text/css'>\n",
       "<style>\n",
       "    @font-face {\n",
       "        font-family: \"Computer Modern\";\n",
       "        src: url('http://mirrors.ctan.org/fonts/cm-unicode/fonts/otf/cmunss.otf');\n",
       "    }\n",
       "    div.cell{\n",
       "        /*width:80%;*/\n",
       "        /*margin-left:auto !important;\n",
       "        margin-right:auto;*/\n",
       "    }\n",
       "    h1 {\n",
       "        font-family: 'Alegreya Sans', sans-serif;\n",
       "    }\n",
       "    h2 {\n",
       "        font-family: 'Fenix', serif;\n",
       "    }\n",
       "    h3{\n",
       "\t\tfont-family: 'Fenix', serif;\n",
       "        margin-top:12px;\n",
       "        margin-bottom: 3px;\n",
       "       }\n",
       "\th4{\n",
       "\t\tfont-family: 'Fenix', serif;\n",
       "       }\n",
       "    h5 {\n",
       "        font-family: 'Alegreya Sans', sans-serif;\n",
       "    }\t   \n",
       "    div.text_cell_render{\n",
       "        font-family: 'Alegreya Sans',Computer Modern, \"Helvetica Neue\", Arial, Helvetica, Geneva, sans-serif;\n",
       "        line-height: 1.2;\n",
       "        font-size: 120%;\n",
       "        /*width:70%;*/\n",
       "        /*margin-left:auto;*/\n",
       "        margin-right:auto;\n",
       "    }\n",
       "    .CodeMirror{\n",
       "            font-family: \"Source Code Pro\";\n",
       "\t\t\tfont-size: 90%;\n",
       "    }\n",
       "/*    .prompt{\n",
       "        display: None;\n",
       "    }*/\n",
       "    .text_cell_render h1 {\n",
       "        font-weight: 200;\n",
       "        font-size: 50pt;\n",
       "\t\tline-height: 110%;\n",
       "        color:#CD2305;\n",
       "        margin-bottom: 0.5em;\n",
       "        margin-top: 0.5em;\n",
       "        display: block;\n",
       "    }\t\n",
       "    .text_cell_render h5 {\n",
       "        font-weight: 300;\n",
       "        font-size: 16pt;\n",
       "        color: #CD2305;\n",
       "        font-style: italic;\n",
       "        margin-bottom: .5em;\n",
       "        margin-top: 0.5em;\n",
       "        display: block;\n",
       "    }\n",
       "    \n",
       "    li {\n",
       "        line-height: 110%;\n",
       "    }\n",
       "    .warning{\n",
       "        color: rgb( 240, 20, 20 )\n",
       "        }  \n",
       "\n",
       "</style>\n",
       "\n",
       "<script>\n",
       "    MathJax.Hub.Config({\n",
       "                        TeX: {\n",
       "                           extensions: [\"AMSmath.js\"]\n",
       "                           },\n",
       "                tex2jax: {\n",
       "                    inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ],\n",
       "                    displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ]\n",
       "                },\n",
       "                displayAlign: 'center', // Change this to 'center' to center equations.\n",
       "                \"HTML-CSS\": {\n",
       "                    styles: {'.MathJax_Display': {\"margin\": 4}}\n",
       "                }\n",
       "        });\n",
       "</script>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "def css_styling():\n",
    "    styles = open(\"./styles/custom.css\", \"r\").read()\n",
    "    return HTML(styles)\n",
    "css_styling()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
